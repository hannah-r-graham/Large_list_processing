{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#what this file does: \n",
    "\n",
    "#1. Takes terms that are already parsed by json_parse_script.ipynb",
    "#2. make everything lower case; get rid of extra characters; ensure certain words like azure and office is followed by a space or a comma\n",
    "#3. put all skill in row (uniques) with counts and users with that skill\n",
    "#4. run spell check on uniques terms "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "initial py speller: 33 sec for every 100 rows. \n",
    "After splitting, 38 sec for every 1000 rows. \n",
    "\n",
    "after spell checking on keywords, there were 200 less unique skills than before (original uniques =7,890 new= 7,632)\n",
    "after spell checking on recruit, improved by 69 less unique skills (original = 2469 new = 2403)\n",
    "after spell checking on ask me about, improved by x (original = 4459 new = 4305)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "pip install cython\n",
    "pip install hdbscan\n",
    "pip install pandas\n",
    "pip install BERTopic\n",
    "pip install pyspellchecker\n",
    "\n",
    "import hdbscan\n",
    "from sklearn.datasets import make_blobs"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "from bertopic import BERTopic\n",
    "from spellchecker import SpellChecker\n",
    "import pickle\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Import files already parsed: advisors/skills, projects/keywords/recruiting\n",
    "\n",
    "skills_file = pd.read_csv('csv_file)\n",
    "\n",
    "\n",
    "#split files into data frames. Rename columns on import.\n",
    "skills = pd.DataFrame(data = skills_file, columns =['alias', 'C'])\n",
    "skills.rename(columns = {\"alias\":\"user\",\n",
    "                \"C\":\"skills\"}, inplace = True)\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# modify input_file variable to select file to be cleaned\n",
    "input_data = pd.DataFrame(skills_file)\n",
    "\n",
    "#column_to_clean = column to be cleaned\n",
    "column_to_clean = skills_file['skills']\n",
    "\n",
    "#df = file to be cleaned\n",
    "# df = pd.DataFrame(input_file)\n",
    "input_data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "input_data['Cleaned'] = clean_column(column_to_clean)\n",
    "input_data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#take input data, split apart, put unique elements into list, count how many entities have that element, ie column 1 = azure, column 2 = user1, user2... then run spell check on it.\n",
    "\n",
    "dfx = process_skills_data(input_data)\n",
    "dfx"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#run spell check by itself on the terms\n",
    "\n",
    "newdf = dfx.drop(['user'], axis = 1)\n",
    "\n",
    "y = spell_check_column(newdf)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#save the old terms and the corrrected terms to a csv\n",
    "\n",
    "y.to_csv('skills_spell_checked.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#use this function to check in after each improvement to the code to understand how many unique skills there are - goal of spell check and reg ex expressions in clean column \n",
    "#was to reduce the amount of skills/terms as a whole. \n",
    "\n",
    "z_count = pd.unique(y['Spelled'])\n",
    "z_unique = 0\n",
    "\n",
    "for item in z_count:\n",
    "    z_unique += 1\n",
    "\n",
    "print(z_unique)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Function to clean: extra characters, spaces, specific words etc\n",
    "\n",
    "def clean_column(column):\n",
    "    # Convert column to lowercase\n",
    "    column = column.str.lower()\n",
    "    \n",
    "    # Remove - and |\n",
    "    column = column.apply(lambda x: re.sub(r'[-|\\||/|\"|“|”|#\\@|(|)]', ' ', str(x)))\n",
    "\n",
    "    # Remove any instance of two in a row of . & % and any double space\n",
    "    column = column.apply(lambda x: re.sub(r'[\\.\\&\\%]{2,}|\\s{2,}]', ' ', str(x)))\n",
    "    \n",
    "    #replace  all !, ', and $ within blank.\n",
    "    column = column.apply(lambda x: re.sub(r'[!\\'\\$]', '', str(x)))\n",
    "\n",
    "    #replace any double space up to 5 spaces with a comma and one space\n",
    "    column = column.apply(lambda x: re.sub(r'([ ]{2,5})', ', ', str(x)))\n",
    "\n",
    "    # Remove &, and, replace with ','\n",
    "    column = column.apply(lambda x: re.sub(r'&|and', ',', str(x)))\n",
    "\n",
    "    #if azure or office has a letter right after or before, insert a space. \n",
    "    column = column.apply(lambda x: re.sub(r'[a-zA-Z]+azure|azure[a-zA-Z]+', ' ', str(x)))\n",
    "\n",
    "    column = column.apply(lambda x: re.sub(r'[a-zA-Z]+office|office[a-zA-Z]+', ' ', str(x)))\n",
    "    \n",
    "    return column"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# the below function takes the output Cleaned column from the function clean_column and modifies the data to have the skills be unique as rows.\n",
    "#  it also counts how many users are list that skill and then lists those users\n",
    "\n",
    "#Variables: \n",
    "#df is file to be cleaned, assigned above. \n",
    "#Cleaned = column processed in previous step (removing &'s etc)\n",
    "#columns lose name after 1st step, so columns need to be renamed. Cannot delete names in step 2\n",
    "\n",
    "def process_skills_data(df):\n",
    "    #1. set user info as index, access cleaned column and split column info on ,, then expand into seperate columns, \n",
    "    # then put columns into rows. Level column created in this step indicates number/place within string. IE hannah: skills: ai, ml level 2 = ml\n",
    "    df1 = df.set_index(\"user\").Cleaned.str.split(',', expand=True).stack().reset_index()\n",
    "\n",
    "    # 2. Rename the columns for clarity\n",
    "    df1.columns = [\"user\", \"level\", \"Cleaned\"]\n",
    "\n",
    "    # 3. Remove any whitespace around the skill name in column cleaned\n",
    "    df1[\"Cleaned\"] = df1[\"Cleaned\"].str.strip()\n",
    "\n",
    "    # 4. Group by the skill name in column Cleaned and find all unique users with that skill\n",
    "    result = df1.groupby(\"Cleaned\").user.unique()\n",
    "    \n",
    "\n",
    "    # 5. CleanCleanColumnolumnonvert the result to a dataframe and add a count column for the number of users with each skill\n",
    "    result = result.to_frame().reset_index()\n",
    "\n",
    "    #this takes a blank string, converts to NA then drops it from data frame. \n",
    "    result.replace(r'^s*$', float('NaN'), regex = True, inplace = True)\n",
    "    result.dropna(inplace = True) \n",
    "\n",
    "    #this line counts how many users listed that skill.\n",
    "    result[\"Count\"] = result[\"user\"].apply(lambda x: len(x))\n",
    "\n",
    "    \n",
    "\n",
    "    return result\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# take a column,\n",
    "# split it into elements, \n",
    "# then run spell check on those Elements\n",
    "# put correct elements in seperate column, do nothing if word is spelled correctly \n",
    "import time\n",
    "\n",
    "def spell_check_column(df):\n",
    "    spell = SpellChecker()\n",
    "    spell.word_frequency.load_text_file('known_words.txt')\n",
    "\n",
    "    def apply_spell_correction(wordList):\n",
    "        updatedWordList = []\n",
    "        words = wordList[0].split()\n",
    "\n",
    "        for word in words:\n",
    "            if len(word) > 3:\n",
    "                newWord = spell.correction(word)\n",
    "                if type(newWord) == type(None):\n",
    "                    updatedWordList.append(word)\n",
    "                else:\n",
    "                    updatedWordList.append(newWord)    \n",
    "            else: \n",
    "                updatedWordList.append(word)\n",
    "        return ' '.join(updatedWordList)\n",
    "    \n",
    "\n",
    "    df['Spelled'] = df['Cleaned'].astype(str)\n",
    "\n",
    "    df['Spelled'] = df['Spelled'].str.split(',', expand = False)\n",
    "\n",
    "    df['Spelled'] = df['Spelled'].apply(apply_spell_correction)\n",
    "    \n",
    "   \n",
    "    \n",
    "    result = df\n",
    "    return(result)\n",
    " "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Begin topicizing elements here. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#read csv of spell checked elements that you want to topic ize\n",
    "\n",
    "df1 = pd.read_csv('spell_checked_terms.csv')\n",
    "\n",
    "df = df1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#load data to df, convert all elements to string.\n",
    "\n",
    "df = pd.DataFrame(data = df2)\n",
    "df = df[['Spelled']].astype(str)\n",
    "df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#take df, convert column to be bucketized to list. \n",
    "\n",
    "data = df\n",
    "docs = data[\"Spelled\"].to_list()\n",
    "\n",
    "#suggest topics to BerTopic to nudge it in the right direction\n",
    "seed_topic_list = [[\"azure\", \"m365\", \"email\", \"dynamics\", \n",
    "                \"security\"]]\n",
    "\n",
    "\n",
    "#Give model your parametersVerbose= explain step by step to developer. nr topics means only give me this many topics \n",
    "topic_model = BERTopic(seed_topic_list = seed_topic_list, verbose = True, nr_topics=20)\n",
    "\n",
    "#train the model given the above info. \n",
    "topics, probs = topic_model.fit_transform(docs)\n",
    "# topics_per_class = topic_model.topics_per_class(docs, classes = classes)\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#get topics groupings from model\n",
    "topic_model.get_topic_info()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#get topics from model and get corresponding topics = see relationship over different levels of granularity of topics.\n",
    "hierarchical_topics = topic_model.hierarchical_topics(docs)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#createh ierarchy of topics ; what sub topics are within the major 20 topics?\n",
    "\n",
    "topic_model.visualize_hierarchy(hierarchical_topics=hierarchical_topics)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#visualize hierarchy of topics \n",
    "\n",
    "tree = topic_model.get_topic_tree(hierarchical_topics)\n",
    "tree"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#visualize topics in barchart\n",
    "topic_model.visualize_barchart()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#display topic goup 9\n",
    "topic_model.get_topic(9)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "topic_model.visualize_topics()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "--------------------------------------------------------\n",
    "MISC\n",
    "--------------------------------------------------------"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# This function takes a word and adds it to the txt file known words for spell checker to reference\n",
    "   \n",
    "# def add_known_word(word):\n",
    "#         with open(\"known_words.txt\", \"a\") as file:\n",
    "#             file.write(word + \"\\n\")\n",
    "        "
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit"
  },
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
